There is a gap between data science and engineering

MLOps or machine learning operations.


Many organizations struggle to push their AI models beyond the training stages because the teams that train these models lack the tools to deploy, deliver, and maintain AI-based systems in production. To solve this problem, RHOAI provides tools such as stable working environments, continuous integration and continuous deployment (CI/CD) pipelines, and model serving frameworks.

With these tools, users can operationalize their ML workflows and achieve reproducibility, boost automation, better collaboration, cost savings, and improved governance and compliance.

Complex ML working environments

    AI practitioners often struggle to integrate and maintain the myriad of tools, libraries, and versions required for data science and machine learning, including the significant effort required to configure GPU support and drivers and keep those updated. RHOAI provides data scientists with ready-to-use working environments that are preconfigured with standard AI/ML tools and libraries.


## Key Features

### Jupyter notebooks
- RHOAI implements a feature called workbenches which essentially are containerized hosting environments for Jupyter notebooks.
- - These can be ran on any of the pre-built AI ready images RedHat provides that come with common AI/ML libraries.
- - - Users can still choose to build their own images if need be.


### gpu ready environments

- So far I only see Nvidia and Intel as accelerator options.

### Cloud-firs Model serving

### OpenShift pipelines

### Model performance monitoring






## Workflow without OpenShift

First a  user would need a namespace
then they need to build a contianer image with Jupyter project environment as well as identify and install any libraries they need.
- pytorch, tesnorflow, pandas, numpy, matplotlib, scikitlearn, etc..... 

2. Then  the user needs to verify with the platform team that there are available accelerators and that the required operators are installed to support them.
- ex. Nvidia GPU operator

3. From there a  user needs to properly define the use of a gpu in there deployments pod spec.

4. They also would need to define cluster storage to keep their work persistent, in addition to provisioning their own S3 or artifact storage for models.

5. Finally they can start doing data science, however imagine expanding this across multiple teams, with different methods and images and versions of software It could get messy fast not to mention the duplicated effort across teams just to get up and running. 

### Data science

Lets assume someone is just trying to prepare a RAG(Retrieval Augmented Generation) for an LLM use case to provide access to provide historical domain knowledge to a team or deparment like Law or HR.

They go through all the steps in their Jupyter environment and are ready to start serving, now they need to potentially repeat the steps from before to get a container ready that has all the tools for deploying the models, as well as hosting the vector database for the aquired domain knowledge

Once they  complete all those steps they are ready to host the RAG.

Now though there is no repeatable easy to use process to run the notebook again and redeploy the rag.





